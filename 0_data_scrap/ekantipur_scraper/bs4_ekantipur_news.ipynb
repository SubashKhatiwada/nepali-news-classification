{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ab5371-b7c7-40ab-ad9c-861a5cb3f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  लुम्बिनी र पशुपति विकास कोषलाई गौरवको आयोजनाबा...   \n",
      "1  साना र मझौला आयोजना संघीय सरकारको बजेटमा नराख्...   \n",
      "2  उच्चस्तरीय आर्थिक सुधार सुझाव आयोगको प्रतिवेदन...   \n",
      "3    अर्थतन्त्र सुधारका लागि प्रतिवेदन बुझाउँदै आयोग   \n",
      "4  ‘आर्थिक विकासका लागि सरकार र निजी क्षेत्र सहका...   \n",
      "\n",
      "                                                 URL            Date  \\\n",
      "0  https://ekantipur.com/business/2025/04/11/sugg...  Date not found   \n",
      "1  https://ekantipur.com/business/2025/04/11/smal...  Date not found   \n",
      "2  https://ekantipur.com/business/2025/04/11/the-...  Date not found   \n",
      "3  https://ekantipur.com/business/2025/04/11/comm...  Date not found   \n",
      "4  https://ekantipur.com/business/2025/04/11/gove...  Date not found   \n",
      "\n",
      "                Author                                 Author URL  \\\n",
      "0  कान्तिपुर संवाददाता  https://ekantipur.com/author/author-14301   \n",
      "1  कान्तिपुर संवाददाता  https://ekantipur.com/author/author-14301   \n",
      "2  कान्तिपुर संवाददाता  https://ekantipur.com/author/author-14301   \n",
      "3         यज्ञ बञ्जाडे  https://ekantipur.com/author/author-20161   \n",
      "4  कान्तिपुर संवाददाता  https://ekantipur.com/author/author-14301   \n",
      "\n",
      "                                             Content  Category  \\\n",
      "0  Facebook Messenger Whatsapp Viber Twiter Copy ...  business   \n",
      "1  Facebook Messenger Whatsapp Viber Twiter Copy ...  business   \n",
      "2  Facebook Messenger Whatsapp Viber Twiter Copy ...  business   \n",
      "3  Facebook Messenger Whatsapp Viber Twiter Copy ...  business   \n",
      "4  Facebook Messenger Whatsapp Viber Twiter Copy ...  business   \n",
      "\n",
      "                                         Description  \n",
      "0  पूर्व अर्थ सचिव रामेश्वर खनालको संयोजकत्वमा गठ...  \n",
      "1  प्रदेश सरकार र स्थानीय तहबाट कार्यान्वयन हुने ...  \n",
      "2  उच्चस्तरीय आर्थिक सुधार आयोगको प्रतिवेदन अर्थम...  \n",
      "3  उच्चस्तरीय आर्थिक सुधार सुझाव आयोगले उपप्रधान ...  \n",
      "4  प्रधानमन्त्री केपी शर्मा ओलीले देशको आर्थिक वि...  \n",
      "Saved article to ekantipur_news/business/ल_म_ब_न_ र पश_पत_ व_क_स क_षल_ई ग_रवक_ आय_जन_ब_ट हट_0.txt\n",
      "Saved article to ekantipur_news/business/स_न_ र मझ_ल_ आय_जन_ स_घ_य सरक_रक_ बज_टम_ नर_ख_न स__1.txt\n",
      "Saved article to ekantipur_news/business/उच_चस_तर_य आर_थ_क स_ध_र स_झ_व आय_गक_ प_रत_व_दन अर__2.txt\n",
      "Saved article to ekantipur_news/business/अर_थतन_त_र स_ध_रक_ ल_ग_ प_रत_व_दन ब_झ_उ_द_ आय_ग_3.txt\n",
      "Saved article to ekantipur_news/business/_आर_थ_क व_क_सक_ ल_ग_ सरक_र र न_ज_ क_ष_त_र सहक_र_य _4.txt\n",
      "Saved article to ekantipur_news/business/५_५७ अ_कल_ घट_य_ न_प_स__5.txt\n",
      "Saved article to ekantipur_news/business/यस_त_ छ आजक_ ल_ग_ न_र_ध_र_त व_द_श_ म_द_र_क_ व_न_मय_6.txt\n",
      "Saved article to ekantipur_news/business/आग_म_ आर_थ_क वर_ष ज_ड_प_क_ ५___५ प_रत_शत आन_तर_क ऋ_7.txt\n",
      "Saved article to ekantipur_news/business/दक_ष जनशक_त_ उत_प_दनम_ क_न_द_र_त छ__ _ श_रममन_त_र__8.txt\n",
      "Saved article to ekantipur_news/business/न_प_स_ स_ढ_ ५ अ_कल_ घट_य__ क_र_ब_र रकम बढ_य__9.txt\n",
      "Saved article to ekantipur_news/business/न_त_गत स_ध_रल_ म_ल_कम_ लग_न_क_ व_त_वरण बन_क_ छ _ उ_10.txt\n",
      "Saved article to ekantipur_news/business/ब_ह_न उठ_न_ब_त_त_क_ न_प_ल_ मञ_जनल_ द__त म_झ_न प_इय_11.txt\n",
      "Saved article to ekantipur_news/business/परर_ष_ट_रमन_त_र_ र_ण_ भन_छ_न_ _ व_द_श_क सह_यत_ घट__12.txt\n",
      "Saved article to ekantipur_news/business/व_र_ष_क ५ ल_खल_ई र_जग_र द_न सक_न_ क_षमत_ न_ज_ क_ष__13.txt\n",
      "Saved article to ekantipur_news/business/अन_श_स_त ढ_गल_ स_र_त व_यवस_थ_पन गर_र बज_ट ल_य_इन_छ_14.txt\n",
      "Saved article to ekantipur_news/business/अव_ध क_र_ब_र न_यन_त_रण गर_न म_रङ उद_य_ग व_य_प_र स__15.txt\n",
      "Saved article to ekantipur_news/business/द_श_द_शम_ ख_द_य_न_न र ब_उ भण_ड_रणल_ई स_य_क_त ब__क_16.txt\n",
      "Saved article to ekantipur_news/world/न_इट क_लबक_ छ_न_ भत_क__द_ मर_न_क_ स_ख_य_ २०० न_घ_य_17.txt\n",
      "Saved article to ekantipur_news/world/ट_रम_पल_ ल_य_एक_ नय__ भन_स_र दर च_नब_ह_क अर_ द_शक__18.txt\n",
      "Saved article to ekantipur_news/world/च_नल_ ज_र_ गर__य_ आफ_न_ न_गर_कक_ ल_ग_ एडभ_इजर__ अम_19.txt\n",
      "Saved article to ekantipur_news/world/ग_ज_क_ आव_स_य भवनम_ इजर_यलक_ आक_रमण_ २९ क_ म_त_य__20.txt\n",
      "Saved article to ekantipur_news/world/अम_र_क__च_न व_य_प_र य_द_ध _ च_नक_ जव_फ_ भन_स_र ८४ _21.txt\n",
      "Saved article to ekantipur_news/world/न_इट क_लबक_ छ_न_ भत_क__द_ कम_त_म_ ११३ जन_क_ म_त_य__22.txt\n",
      "Saved article to ekantipur_news/world/य_क_र_नम_ १ कर_ड ३० ल_ख म_न_सल_ई तत_क_ल म_नव_य सह__23.txt\n",
      "Saved article to ekantipur_news/world/औषध_म_ पन_ ट_य_र_फ लग_उन_ ट_रम_पक_ य_जन__24.txt\n",
      "Saved article to ekantipur_news/world/उत_तर_ च_नक_ नर_स_ङ ह_मम_ आगल_ग__ २० जन_क_ म_त_य__25.txt\n",
      "Saved article to ekantipur_news/world/१०४ प_रत_शत ट_य_र_फपछ_ च_नक_ सरक_र_ म_ड_य_ल_ द_य_ _26.txt\n",
      "Saved article to ekantipur_news/sports/लल_तप_र स_ट_ प_ल_अफत_र_27.txt\n",
      "Saved article to ekantipur_news/sports/द_ल_ल_क_ च_थ_ ज_त_28.txt\n",
      "Saved article to ekantipur_news/sports/ह_म_ल र इभ_न_ ट_बलट_न_स च_य_म_प_यन_29.txt\n",
      "Saved article to ekantipur_news/sports/ओसन कप य__१३ फ_टबल स_र__30.txt\n",
      "Saved article to ekantipur_news/sports/ल_म_ब_न_ प_रद_शस_तर_य क_र_क_टम_ द_ङ र र_पन_द_ह_ स__31.txt\n",
      "Saved article to ekantipur_news/sports/क_प_ ओल_ कप क_र_क_ट _ गण_डक_ ह_र_ज स_म_फ_इनलम__32.txt\n",
      "Saved article to ekantipur_news/sports/ट_-२० आईम_ द_प_न_द_रक_ अर_क_ व_श_व क_र_त_म_न_33.txt\n",
      "Saved article to ekantipur_news/sports/हङकङ स_र_ज_ न_प_लक_ लग_त_र द_स_र_ ज_त_ क_व_त ६ व_क_34.txt\n",
      "Saved article to ekantipur_news/sports/क_व_तव_र_द_ध भ_र_त_लपछ_ आश_फक_ पन_ अर_धशतक_35.txt\n",
      "Saved article to ekantipur_news/sports/क_व_तव_र_द_ध भ_र_त_लक_ अर_धशतक_36.txt\n",
      "Saved article to ekantipur_news/sports/हङकङ स_र_ज_ क_व_तल_ द_य_ न_प_लल_ई १८६ रनक_ च_न_त_प_37.txt\n",
      "Saved article to ekantipur_news/sports/ज_तप_र स_मर_ ग_ल_डकप _ ह_म_लयन श_र_प_ फ_इनलम__38.txt\n",
      "Saved article to ekantipur_news/sports/न_प_ल स_पर ल_ग _ ग_लरह_त बर_बर_ द_व_ल_ई घ_ट__39.txt\n",
      "Saved article to ekantipur_news/sports/र_जस_थ_नल_ई हर_एर ग_जर_त श_र_षम__40.txt\n",
      "Saved article to ekantipur_news/sports/हङकङम_ न_प_ल व_जय__41.txt\n"
     ]
    }
   ],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Set up PoolManager with headers\n",
    "http = urllib3.PoolManager()\n",
    "http.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "\n",
    "# Initialize dictionary to store news data\n",
    "ndict = {\n",
    "    'Title': [], \"URL\": [], \"Date\": [],\n",
    "    \"Author\": [], \"Author URL\": [], \"Content\": [],\n",
    "    \"Category\": [], \"Description\": []\n",
    "}\n",
    "\n",
    "# Define categories and their URLs\n",
    "categories = {\n",
    "    \"business\": \"https://ekantipur.com/business/\",\n",
    "    \"world\": \"https://ekantipur.com/world/\",\n",
    "    \"sports\": \"https://ekantipur.com/sports/\"\n",
    "}\n",
    "\n",
    "show = False  # Toggle to True if you want to print each entry\n",
    "\n",
    "# Loop through each category\n",
    "for category, url in categories.items():\n",
    "    web_page = http.request('GET', url)\n",
    "    soup = BS(web_page.data, 'html.parser')\n",
    "\n",
    "    # Loop through all divs with '.normal' class\n",
    "    for row in soup.select(\".normal\"):\n",
    "        # Title is in an h2 element\n",
    "        title = row.find(\"h2\")\n",
    "\n",
    "        # Skip if no title or link is found\n",
    "        if title and title.a:\n",
    "            # Get title text\n",
    "            title_text = title.text.strip()\n",
    "            \n",
    "            # Extract the href attribute and construct full URL\n",
    "            title_link = title.a.get(\"href\")\n",
    "            if title_link.split(\":\")[0] != \"https\":\n",
    "                title_link = url.split(f\"/{category}\")[0] + title_link\n",
    "\n",
    "            # Description is in a p element\n",
    "            description = row.find(\"p\")\n",
    "            description_text = description.text.strip() if description else \"No description available\"\n",
    "\n",
    "            # Request the individual news page\n",
    "            news_page = http.request('GET', title_link)\n",
    "            news_soup = BS(news_page.data, 'html.parser')\n",
    "\n",
    "            # Find the date (with fallback)\n",
    "            date_elem = news_soup.find(\"time\")\n",
    "            date = date_elem.text.strip() if date_elem else \"Date not found\"\n",
    "\n",
    "            # Find the author URL and name (with fallback)\n",
    "            author_elem = news_soup.select_one(\".author\")\n",
    "            if author_elem and author_elem.a:\n",
    "                author_url = author_elem.a.get(\"href\")\n",
    "                author_name = author_elem.text.strip()\n",
    "            else:\n",
    "                author_url = \"Author URL not found\"\n",
    "                author_name = \"Unknown Author\"\n",
    "\n",
    "            # Find the news content (with fallback)\n",
    "            news_content = \"\"\n",
    "            content_container = news_soup.select_one(\".row\")\n",
    "            if content_container:\n",
    "                for content in content_container.findAll(\"p\"):\n",
    "                    content_parts = str(content).split(\">\")\n",
    "                    if len(content_parts) > 1:\n",
    "                        content_text = content_parts[1].split(\"<\")[0].strip()\n",
    "                        if len(content_text) == 0:\n",
    "                            break\n",
    "                        else:\n",
    "                            news_content += content_text + \" \"\n",
    "                content = news_content.strip()\n",
    "            else:\n",
    "                content = \"Content not found\"\n",
    "\n",
    "            # Append data to dictionary\n",
    "            ndict[\"Title\"].append(title_text)\n",
    "            ndict[\"URL\"].append(title_link)\n",
    "            ndict[\"Date\"].append(date)\n",
    "            ndict[\"Author\"].append(author_name)\n",
    "            ndict[\"Author URL\"].append(author_url)\n",
    "            ndict[\"Content\"].append(content)\n",
    "            ndict[\"Category\"].append(category)\n",
    "            ndict[\"Description\"].append(description_text)\n",
    "\n",
    "            # Optional: Print the entry\n",
    "            if show:\n",
    "                print(f\"\"\"\n",
    "                    Title: {title_text}, URL: {title_link}\n",
    "                    Date: {date}, Author: {author_name}, Category: {category},\n",
    "                    Author URL: {author_url},\n",
    "                    Description: {description_text},\n",
    "                    Content: {content}\n",
    "                \"\"\")\n",
    "        else:\n",
    "            print(f\"Skipping an entry in {category} due to missing title or link\")\n",
    "\n",
    "# Create DataFrame\n",
    "ekantipur_df = pd.DataFrame(ndict, columns=list(ndict.keys()))\n",
    "\n",
    "# Display the DataFrame head (optional)\n",
    "print(ekantipur_df.head())\n",
    "\n",
    "# Save each article to a unique .txt file in category-wise folders\n",
    "base_dir = \"ekantipur_news\"  # Base directory to store category folders\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "# Loop through each category\n",
    "for category in categories.keys():\n",
    "    # Create category folder if it doesn't exist\n",
    "    category_dir = os.path.join(base_dir, category)\n",
    "    if not os.path.exists(category_dir):\n",
    "        os.makedirs(category_dir)\n",
    "    \n",
    "    # Filter DataFrame for this category\n",
    "    category_df = ekantipur_df[ekantipur_df[\"Category\"] == category]\n",
    "    \n",
    "    # Save each article in a separate file\n",
    "    for index, row in category_df.iterrows():\n",
    "        # Create a safe filename from the title (replace invalid chars)\n",
    "        safe_title = \"\".join(c if c.isalnum() or c in \" _-\" else \"_\" for c in row['Title'])[:50]\n",
    "        file_name = f\"{safe_title}_{index}.txt\"  # Use index to ensure uniqueness\n",
    "        file_path = os.path.join(category_dir, file_name)\n",
    "        \n",
    "        # Write to text file\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Title: {row['Title']}\\n\")\n",
    "            f.write(f\"URL: {row['URL']}\\n\")\n",
    "            f.write(f\"Date: {row['Date']}\\n\")\n",
    "            f.write(f\"Author: {row['Author']}\\n\")\n",
    "            f.write(f\"Author URL: {row['Author URL']}\\n\")\n",
    "            f.write(f\"Category: {row['Category']}\\n\")\n",
    "            f.write(f\"Description: {row['Description']}\\n\")\n",
    "            f.write(f\"Content: {row['Content']}\\n\")\n",
    "        \n",
    "        print(f\"Saved article to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beebfe24-2a90-4354-a5ea-f195d37bd716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedbd05-18ee-40fd-b912-c57ebfae2070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
