{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a80ea56c-b14d-47ea-bab2-6c25eb3fcfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f85872e0-20c2-4550-b4c8-f4a392f94c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb9a14cf-2486-44bf-a791-78b9aa06d50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check if the MPS (Apple's GPU) backend is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb23e845-65ff-45df-b770-3fe11bab7cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['content', 'label'],\n",
      "        num_rows: 1625\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['content', 'label'],\n",
      "        num_rows: 434\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['content', 'label'],\n",
      "        num_rows: 650\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset from CSV file\n",
    "train_data = load_dataset('csv', data_files='3_split_data/train.csv')\n",
    "test_data = load_dataset('csv', data_files='3_split_data/test.csv')\n",
    "validation_data = load_dataset('csv', data_files='3_split_data/validation.csv')\n",
    "dataset = DatasetDict({\n",
    "    'train': train_data['train'],\n",
    "    'test': test_data['train'],\n",
    "    'validation':validation_data['train']\n",
    "})\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d8ad56a-2ffa-4c63-8f35-72d95c95a153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362161ec663c496ba8552d5a5e765540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1625 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3357853f2fcd4650baab2a9fe725e6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/434 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fdb509f5f246ecaeffa0b8d1dcec3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['content', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1625\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['content', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 434\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['content', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 650\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['content'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63c9a353-6e76-46ad-9de3-42138dc6236b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased', num_labels=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb738a0-cf74-4acc-bb2d-531d896f85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",   # Evaluate at the end of each epoch\n",
    "    per_device_train_batch_size=8, # Adjust batch size as needed\n",
    "    per_device_eval_batch_size=8,  # Adjust batch size as needed\n",
    "    num_train_epochs=5,            # Number of training epochs\n",
    "    weight_decay=0.01,             # Strength of weight decay\n",
    "    logging_dir='./logs',          # Directory for logging\n",
    "    logging_steps=10,              # Log every 10 steps\n",
    "    bf16=True                      # Use BF16 precision for MPS compatibility\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-tune and evaluate the model\n",
    "trainer.train()\n",
    "\n",
    "# Retrieve training history\n",
    "training_history = trainer.state.log_history\n",
    "\n",
    "# Extract evaluation metrics and losses after each epoch\n",
    "epochs = []\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "training_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "for log in training_history:\n",
    "    if 'eval_accuracy' in log:  # Log evaluation metrics\n",
    "        epochs.append(log['epoch'])\n",
    "        accuracies.append(log['eval_accuracy'])\n",
    "        precisions.append(log['eval_precision'])\n",
    "        recalls.append(log['eval_recall'])\n",
    "        f1_scores.append(log['eval_f1'])\n",
    "    if 'loss' in log:  # Log training loss\n",
    "        training_losses.append(log['loss'])\n",
    "    if 'eval_loss' in log:  # Log evaluation loss\n",
    "        eval_losses.append(log['eval_loss'])\n",
    "\n",
    "# Plotting the evaluation metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(epochs, accuracies, label='Accuracy', marker='o')\n",
    "plt.plot(epochs, precisions, label='Precision', marker='o')\n",
    "plt.plot(epochs, recalls, label='Recall', marker='o')\n",
    "plt.plot(epochs, f1_scores, label='F1 Score', marker='o')\n",
    "\n",
    "plt.title('Evaluation Metrics Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the loss values\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(range(1, len(training_losses) + 1), training_losses, label='Training Loss', marker='o')\n",
    "plt.plot(range(1, len(eval_losses) + 1), eval_losses, label='Validation Loss', marker='o')\n",
    "\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6e4ac-0fd7-4df7-82c9-1494cd01ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model('./nepali_news_mbert_fine_tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48dfe8d-4f6f-4d60-b2c9-6a5c7453dda3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e8c0f-ebf4-4a61-9d34-f198eadf1558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
