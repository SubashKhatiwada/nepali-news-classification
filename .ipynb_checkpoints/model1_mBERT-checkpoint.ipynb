{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a80ea56c-b14d-47ea-bab2-6c25eb3fcfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb9a14cf-2486-44bf-a791-78b9aa06d50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "#Checking GPU is avaibale or not\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb23e845-65ff-45df-b770-3fe11bab7cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['content', 'label'],\n",
      "        num_rows: 9856\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['content', 'label'],\n",
      "        num_rows: 2113\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['content', 'label'],\n",
      "        num_rows: 2112\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset from CSV file\n",
    "from datasets import load_dataset, DatasetDict\n",
    "train_data = load_dataset('csv', data_files='3_split_data/train.csv')\n",
    "test_data = load_dataset('csv', data_files='3_split_data/test.csv')\n",
    "validation_data = load_dataset('csv', data_files='3_split_data/validation.csv')\n",
    "dataset = DatasetDict({\n",
    "    'train': train_data['train'],\n",
    "    'test': test_data['train'],\n",
    "    'validation':validation_data['train']\n",
    "})\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d8ad56a-2ffa-4c63-8f35-72d95c95a153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263b143a0beb41cda0ff0dffa07878c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c66670d216456abac05114bcb0eec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9083f1bfebc433ea0073df1de1e9623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['content', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 9856\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['content', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 2113\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['content', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 2112\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    contents = [str(content) for content in examples['content']]\n",
    "    return tokenizer(contents, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0a9dd9b-262d-4b55-a9ae-2c4a51204c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train example:\n",
      "Content: भुवन पौडेल काठमाडौं चैत धितोपत्रको बजारमा सानो रकम संकलन लागत महँगिएपछि विकल्प खोज्नुपर्ने देखिएको धितोपत्रको प्राथमिक बजारमार्फत सानो पुँजी संकलन संस्थाले निष्कासन रकमहाराहारीमै खर्च गर्नुपरेपछि विकल्प खोज्नुपर्ने देखिएको पुँजी संकलनका गरिने सेयर निष्कासनको लागत बढेसँगै सम्बन्धित कम्पनीका प्रावधान निल्नु ओकल्नु’ केहीअघि सर्वसाधारणमा सेयर निष्कासन जनउत्थान लघुवित्त विकास बैंकको रुपैयाँबराबरको कित्तामध्ये सामूहिक लगानी कोषलाई छुट्ट्याएपछिको रुपैयाँबराबरको कित्ता सेयरका गुणा आवेदन परेको निष्कासनका कुल रुपैयाँ खर्च सेयर निष्कासन बिक्री प्रबन्धकबीचको सम्झौता खर्च बेहोर्ने सन्दर्भमा बैंक बिक्री प्रबन्धकबीच विवाद आएपछि धितोपत्र बोर्डको मध्यस्थतामा समस्या समाधान बजारको कारोबार खुलेको हालैमात्र साधारण सेयर निष्कासन माइक्रोफाइनान्समा गुणा आवेदन परेको संस्थाले निष्कासन कित्ता सेयरका परेको अधिक आवेदनले खर्च निश्चित माइक्रोफाइनान्सको साधारण सेयर निष्कासनमा परेको अधिक आवेदनअनुसार करिब करोड खर्च लाग्ने धितोपत्रको प्राथमिक बजारबाट सानो रकम संकलन खर्चिनुपरेका उदाहरणमात्र यसअघि साधारण सेयर निष्कासन कम्पनीले धितोपत्र बजारबाट सानो रकम संकलन खोज्दा खर्च गर्नुपरेको सर्वसाधारणमा सेयर जारी पुँजी संकलन सीमा नतोकिएकाले समस्या न्यूनतम रकमसम्मको पुँजी धितोपत्रको प्राथमिक बजारबाट संकलन व्यवस्था नेपाल धितोपत्र बोर्डका प्रवक्ता नीरज गिरीले साना संस्थाको सेयर निष्कासन सन्दर्भमा राष्ट्र बैंक बोर्ड सम्बन्धित नियामक निकायबीच छलफल विकल्प खोज्नुपर्ने “सर्वसाधारणमा माइक्रोफाइनान्सको सन्दर्भमा हालको साधारण सेयर निष्कासनको व्यवस्थामा समीक्षा आवश्यक छ” “साना संस्थाका ओभर द काउन्टर’ ओटीसी सोच्न सकिन्छ ” मर्चेन्ट बैंकर एसोसिएसनका अध्यक्ष प्रवीणरमण पराजुली आकारको कम्पनीको पुँजी संकलन थलो भएकाले बताउँछन् “सानो संस्थाले प्राथमिक बजार प्रयोग खर्च छ” “सेयर जारी संकलन रकमका आधारमा प्ल्याटफर्मको विविधीकरण ” सर्वसाधारणमा नभएर प्राइभेट प्लेसमेन्टमार्फत पुँजी संकलन विकल्पहरू खुला गर्नुपर्ने “हाम्रो प्राइभेट प्लेसमेन्ट डिबेन्चरका प्रयोगमा छ” पराजुलीले “पुँजी संकलन सन्दर्भमा सानो संस्थाका नियामकले सोच्नुपर्छ ” आह्वान गरेजति आवेदन आएपछि बन्द व्यवस्था गरिए खर्च जनाए सानो रकम संकलन संस्थाका प्राइभेट प्लेसमेन्टसँगै ओटीसी मार्केट विकल्प हुनसक्ने विज्ञहरू बताउँछन् विकल्प प्रयोग बजारबाट पुँजी संकलन खर्च पुग्छ त्यसका नेपाल राष्ट्र बैंकले सर्वसाधारणमा सेयर बिक्री नगर्दासम्म लाभांश बाँड्न नपाउने व्यवस्थासँगै अनिवार्य सेयर निष्कासनका प्रावधानहरू ल्याउनुपर्ने उनीहरू बताउँछन् नेपाल धितोपत्र बोर्ड नेपाल स्टक एक्सचेन्जनेप्सेको अनुरोधमा कम्पनी रजिस्ट्रारको कार्यालयले धितोपत्रको बजारमा सूचीकृत नभएका पब्लिक कम्पनीहरूको सेयर ओटीसीमार्फत कारोबार व्यवस्था ओटीसी मार्केट विनियमावलीमा बजारमा सूचीकृत नसकेका नभएका पब्लिक कम्पनीको सेयर कारोबार ओटीसी मार्केटमार्फत व्यवस्था विनियमावलीअनुसार सूचीकृत नभएका सूचीकरणबाट हटाइएका पब्लिक कम्पनीको सेयर कारोबार ओटीसीमार्फत ओटीसी मार्केट काउन्टर मार्केटमा सूचीकरण नसकेका सूचीकरणबाट हटाइएका सर्वसाधारणमा सेयर बिक्री नगरेका पब्लिक कम्पनीको सेयर कारोबार बजार ओटीसी बजार धितोपत्रको बजार सञ्चालक नेप्सेले नियमन नेप्सेले सालमा ओटीसी बजार हालसम्म नेपाल बैंकको कारोबार बजारमा कारोबार नसकेका कम्पनीले ओटीसी मार्केट प्रयोग पाइँदैन बजारमा कारोबार आवश्यक कागजातमात्र पेस कारोबार गरेबापत नेप्सेलाई शुल्क तिर्नुपर्छ थोरै पुँजी संकलन प्राथमिक बजार खर्चिलो\n",
      "Label: 0\n",
      "Input IDs: [101, 570, 34786, 567, 43329, 48299, 11263, 24537, 12572, 12876, 43329, 552, 11354, 565, 13036, 11231, 33578, 94534, 569, 37485, 87723, 579, 16985, 11231, 573, 12030, 12670, 31034, 11059, 93166, 11354, 571, 16080, 13560, 70810, 85579, 53836, 41033, 15674, 548, 72148, 17441, 11059, 76016, 49068, 565, 13036, 11231, 33578, 94534, 61087, 18631, 57785, 569, 37485, 87723, 11551, 20705, 11354, 579, 16985, 11231, 567, 25662, 31034, 11059, 76008, 11468, 566, 13060, 43749, 26540, 11059, 573, 12030, 12670, 50007, 44797, 39209, 12670, 548, 57195, 26106, 17441, 85579, 53836, 41033, 15674, 548, 72148, 17441, 11059, 76016, 49068, 567, 25662, 31034, 11059, 15132, 37092, 21304, 579, 42569, 566, 13060, 43749, 26540, 11059, 12500, 93166, 11354, 569, 30119, 12114, 13560, 579, 12670, 84846, 31885, 11354, 30543, 35078, 52580, 10949, 61087, 14063, 44102, 566, 28393, 11059, 545, 41033, 11059, 100, 57285, 96806, 44670, 13060, 61267, 31031, 76009, 12572, 579, 42569, 566, 13060, 43749, 26540, 11059, 18782, 96808, 11354, 49377, 11059, 574, 44670, 68278, 73225, 23201, 63923, 12500, 23085, 80969, 27843, 24800, 12500, 14500, 73225, 91118, 579, 35877, 84550, 77780, 23067, 11483, 22965, 28669, 553, 75864, 13163, 18155, 85579, 12500, 23085, 80969, 27843, 24800, 12500, 14500, 73225, 10949, 579, 42569, 15132, 549, 14307, 10949, 533, 14063, 31614, 12334, 12500, 566, 13060, 43749, 26540, 11059, 15132, 14372, 23085, 10949, 548, 57195, 579, 42569, 566, 13060, 43749, 26540, 11059, 569, 13559, 16129, 12334, 84846, 19713, 12030, 42003, 14870, 12500, 579, 12670, 66170, 43329, 13142, 548, 57195, 569, 16080, 43908, 11059, 15518, 21099, 32016, 12572, 63923, 569, 13559, 16129, 12334, 84846, 19713, 12030, 42003, 14870, 53836, 47832, 533, 18155, 85579, 565, 13036, 11231, 33578, 11551, 569, 43908, 83957, 11231, 17955, 12114, 18631, 13142, 12572, 579, 49611, 13163, 579, 12572, 44102, 569, 37485, 94534, 11263, 36666, 94938, 56426, 12500, 92643, 12572, 18216, 579, 10949, 76009, 579, 42569, 566, 13060, 43749, 26540, 11059, 29047, 33912, 19086, 11231, 75067, 35942, 16985, 89565, 549, 14307, 10949, 533, 14063, 31614, 12334, 12500, 76008, 11468, 566, 13060, 43749, 26540, 11059, 14500, 73225, 10949, 579, 42569, 15132, 12334, 12500, 23974, 533, 14063, 31614, 11468, 548, 57195, 566, 93221, 29047, 33912, 19086, 11231, 75067, 35942, 16985, 12114, 12500, 579, 10949, 76009, 579, 42569, 566, 13060, 43749, 26540, 11059, 12572, 12334, 12500, 23974, 533, 14063, 31614, 96806, 89906, 17277, 15807, 13060, 17046, 15807, 38659, 548, 57195, 93166, 11059, 565, 13036, 11231, 33578, 94534, 61087, 18631, 57785, 569, 37485, 11551, 48995, 579, 16985, 11231, 573, 12030, 12670, 31034, 11059, 548, 57195, 21304, 17441, 15132, 72156, 12572, 18216, 29357, 96806, 44670, 13060, 579, 10949, 76009, 579, 42569, 566, 13060, 43749, 26540, 11059, 30543, 35078, 28026, 565, 13036, 11231, 33578, 11551, 569, 37485, 11551, 48995, 579, 16985, 11231, 573, 12030, 12670, 31034, 11059, 548, 70238, 21671, 548, 57195, 26106, 17441, 12500, 61267, 31031, 76009, 12572, 579, 42569, 70345, 567, 25662, 31034, 11059, 90735, 566, 36309, 78302, 64240, 11468, 579, 49611, 13163, 66590, 39706, 12670, 573, 12030, 49611, 76268, 12500, 567, 25662, 565, 13036, 11231, 33578, 94534, 61087, 18631, 57785, 569, 37485, 11551, 48995, 31034, 11059, 70932, 29986, 565, 13036, 11231, 33578, 11551, 569, 43908, 83957, 10949, 19795, 22975, 10949, 566, 72292, 14256, 549, 102]\n",
      "Token Type Ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train example:\")\n",
    "print(\"Content:\", tokenized_dataset['train'][0]['content'])\n",
    "print(\"Label:\", tokenized_dataset['train'][0]['label'])\n",
    "print(\"Input IDs:\", tokenized_dataset['train'][0]['input_ids'])\n",
    "print(\"Token Type Ids:\", tokenized_dataset['train'][0]['token_type_ids'])\n",
    "print(\"Attention Mask:\", tokenized_dataset['train'][0]['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63c9a353-6e76-46ad-9de3-42138dc6236b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased', num_labels=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45a05017-751a-45e4-b278-dff4d6628ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3944' max='3944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3944/3944 8:04:12, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.003000</td>\n",
       "      <td>0.758627</td>\n",
       "      <td>0.753314</td>\n",
       "      <td>0.752718</td>\n",
       "      <td>0.753314</td>\n",
       "      <td>0.739191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.632800</td>\n",
       "      <td>0.622861</td>\n",
       "      <td>0.803504</td>\n",
       "      <td>0.796316</td>\n",
       "      <td>0.803504</td>\n",
       "      <td>0.788927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.471900</td>\n",
       "      <td>0.603405</td>\n",
       "      <td>0.808712</td>\n",
       "      <td>0.808584</td>\n",
       "      <td>0.808712</td>\n",
       "      <td>0.801003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.322100</td>\n",
       "      <td>0.631616</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.825931</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.824115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3944, training_loss=0.6072869911396963, metrics={'train_runtime': 29058.5532, 'train_samples_per_second': 1.357, 'train_steps_per_second': 0.136, 'total_flos': 1.037382158647296e+16, 'train_loss': 0.6072869911396963, 'epoch': 4.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Metric calculation function\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Training Parameters\n",
    "batch_size = 10\n",
    "no_epochs = 4\n",
    "train_len = len(dataset['train'])\n",
    "steps = (train_len * no_epochs) / batch_size\n",
    "\n",
    "# Training Arguments with Optimizations\n",
    "training_args = TrainingArguments(\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    save_strategy=\"epoch\",\n",
    "    output_dir='./results',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=train_len // batch_size,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=no_epochs,\n",
    "    weight_decay=0.05,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=int(0.05 * steps),\n",
    "    # load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"eval_loss\",\n",
    "    # greater_is_better=False,\n",
    "    bf16=True,  # Use fp16 for faster training\n",
    "    # gradient_accumulation_steps=2,  # Reduce memory load and still achieve a larger effective batch size\n",
    "    # gradient_checkpointing=True  # Reduce memory usage\n",
    ")\n",
    "\n",
    "# Trainer initialization with AdamW and custom callbacks\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    optimizers=(AdamW(model.parameters(), lr=training_args.learning_rate), None),  # Faster AdamW optimizer\n",
    "    compute_metrics=compute_metrics,\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]  # Early stopping for fewer epochs if no improvement\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873fb029-1a5b-4334-a4c6-cf72b9abeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve training history\n",
    "training_history = trainer.state.log_history\n",
    "\n",
    "# Extract evaluation metrics and losses after each epoch\n",
    "epochs = []\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "training_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "for log in training_history:\n",
    "    if 'eval_accuracy' in log:  # Log evaluation metrics\n",
    "        epochs.append(log['epoch'])\n",
    "        accuracies.append(log['eval_accuracy'])\n",
    "        precisions.append(log['eval_precision'])\n",
    "        recalls.append(log['eval_recall'])\n",
    "        f1_scores.append(log['eval_f1'])\n",
    "    if 'loss' in log:  # Log training loss\n",
    "        training_losses.append(log['loss'])\n",
    "    if 'eval_loss' in log:  # Log evaluation loss\n",
    "        eval_losses.append(log['eval_loss'])\n",
    "\n",
    "# Plotting the evaluation metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(epochs, accuracies, label='Accuracy', marker='o')\n",
    "plt.plot(epochs, precisions, label='Precision', marker='o')\n",
    "plt.plot(epochs, recalls, label='Recall', marker='o')\n",
    "plt.plot(epochs, f1_scores, label='F1 Score', marker='o')\n",
    "\n",
    "plt.title('Evaluation Metrics Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the loss values\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(range(1, len(training_losses) + 1), training_losses, label='Training Loss', marker='o')\n",
    "plt.plot(range(1, len(eval_losses) + 1), eval_losses, label='Validation Loss', marker='o')\n",
    "\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fd6e4ac-0fd7-4df7-82c9-1494cd01ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model('./nepali_news_mbert_fine_tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2039a42a-8a84-47c1-85a3-5ee38893fbd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
